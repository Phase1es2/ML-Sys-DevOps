import torch
import time
import os
import numpy as np
from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure
from torchmetrics.audio import SignalNoiseRatio
import torch.nn.functional as F
from model_def import DepthProForSuperResolution, get_depthpro_model

# ==== é…ç½®æ¨¡å‹è·¯å¾„ä¸æµ‹è¯•å›¾åƒï¼ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰ ====
model_path = "depthpro_sr_quantized.pt"  # æ›¿æ¢æˆä½ çš„æ¨¡å‹
is_torchscript = model_path.endswith(".pt")

# ==== æ„å»ºæ¨¡å‹ç»“æ„ & åŠ è½½å‚æ•° ====
if is_torchscript:
    model = torch.jit.load(model_path, map_location="cpu")
else:
    depth_model = get_depthpro_model(compressed=True)
    model = DepthProForSuperResolution(depth_model)
    model.load_state_dict(torch.load(model_path, map_location="cpu"))

model.eval()

# ==== æ¨¡å‹å¤§å° ====
model_size = os.path.getsize(model_path) / 1e6
print(f"ğŸ“¦ Model Size on Disk: {model_size:.2f} MB")

# ==== æ¨¡æ‹Ÿè¾“å…¥æ•°æ®ï¼ˆå¦‚æœ‰å®é™…æ•°æ®å¯æ›¿æ¢ï¼‰ ====
B, C, H, W = 4, 3, 64, 64
input_batch = torch.randn(B, C, H, W)
target_batch = torch.randn(B, C, H * 4, W * 4)  # å‡è®¾è¶…åˆ† x4ï¼ŒæŒ‰å®é™…è®¾ç½®æ”¹

# ==== å®šä¹‰æŒ‡æ ‡ ====
mse_fn = torch.nn.MSELoss()
psnr = PeakSignalNoiseRatio().cpu()
ssim = StructuralSimilarityIndexMeasure().cpu()
snr = SignalNoiseRatio().cpu()

# ==== æ¨ç†å¹¶è®¡ç®—å›¾åƒæŒ‡æ ‡ ====
with torch.no_grad():
    preds = model(input_batch)
    preds = F.interpolate(preds, size=target_batch.shape[2:])  # resize

    test_results = {
        "test_mse_loss": mse_fn(preds, target_batch).item(),
        "test_psnr": psnr(preds, target_batch).item(),
        "test_ssim": ssim(preds, target_batch).item(),
        "test_snr": snr(preds, target_batch).item()
    }

# ==== æ¨ç†å»¶è¿Ÿè¯„ä¼°ï¼ˆå•æ ·æœ¬ï¼‰ ====
single_input = input_batch[0:1]
num_trials = 100

# é¢„çƒ­
with torch.no_grad():
    model(single_input)

latencies = []
with torch.no_grad():
    for _ in range(num_trials):
        start = time.time()
        _ = model(single_input)
        latencies.append(time.time() - start)

latencies = np.array(latencies)
print(f"Inference Latency (median): {np.percentile(latencies, 50)*1000:.2f} ms")
print(f"Inference Latency (95th): {np.percentile(latencies, 95)*1000:.2f} ms")
print(f"Inference Latency (99th): {np.percentile(latencies, 99)*1000:.2f} ms")
print(f"Inference Throughput (1 sample): {num_trials/latencies.sum():.2f} FPS")

# ==== æ‰¹é‡æ¨ç†ååè¯„ä¼° ====
num_batches = 50
with torch.no_grad():
    model(input_batch)  # é¢„çƒ­

batch_times = []
with torch.no_grad():
    for _ in range(num_batches):
        start = time.time()
        _ = model(input_batch)
        batch_times.append(time.time() - start)

batch_fps = (input_batch.shape[0] * num_batches) / np.sum(batch_times)

# ==== è¾“å‡ºæ€»ç»“ ====
print("\n===== Summary =====")
print(f"ğŸ“¦ Model Size: {model_size:.2f} MB")
for k, v in test_results.items():
    print(f"ğŸ“Š {k}: {v:.4f}")
print(f"âš¡ Latency (median): {np.percentile(latencies, 50)*1000:.2f} ms")
print(f"âš¡ Latency (95th): {np.percentile(latencies, 95)*1000:.2f} ms")
print(f"âš¡ Latency (99th): {np.percentile(latencies, 99)*1000:.2f} ms")
print(f"ğŸš€ Single-sample FPS: {num_trials/latencies.sum():.2f}")
print(f"ğŸš€ Batch Throughput: {batch_fps:.2f} FPS")