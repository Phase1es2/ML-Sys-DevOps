{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f9bc70f-1a56-4139-9128-e4045c5ab3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoyanouyang/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../mlflow-scripts\")\n",
    "from model import get_depthpro_model, LightningModel\n",
    "from dataloader import get_dataloaders, Urban100Dataset, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93efc775-0cc5-49ed-855d-5beec1881316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DepthProForDepthEstimation were not initialized from the model checkpoint at geetu040/DepthPro and are newly initialized: ['depth_pro.encoder.feature_projection.projections.4.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.3.2.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.4.3.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.5.4.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.6.0.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.6.1.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.6.2.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.6.3.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.6.4.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.6.5.weight', 'fusion_stage.layers.4.deconv.weight', 'fusion_stage.layers.5.projection.bias', 'fusion_stage.layers.5.projection.weight', 'fusion_stage.layers.5.residual_layer1.convolution1.bias', 'fusion_stage.layers.5.residual_layer1.convolution1.weight', 'fusion_stage.layers.5.residual_layer1.convolution2.bias', 'fusion_stage.layers.5.residual_layer1.convolution2.weight', 'fusion_stage.layers.5.residual_layer2.convolution1.bias', 'fusion_stage.layers.5.residual_layer2.convolution1.weight', 'fusion_stage.layers.5.residual_layer2.convolution2.bias', 'fusion_stage.layers.5.residual_layer2.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DepthProForDepthEstimation were not initialized from the model checkpoint at geetu040/DepthPro and are newly initialized because the shapes did not match:\n",
      "- depth_pro.encoder.feature_projection.projections.0.weight: found shape torch.Size([256, 1024, 3, 3]) in the checkpoint and torch.Size([128, 512, 3, 3]) in the model instantiated\n",
      "- depth_pro.encoder.feature_projection.projections.1.weight: found shape torch.Size([256, 1024, 3, 3]) in the checkpoint and torch.Size([128, 256, 3, 3]) in the model instantiated\n",
      "- depth_pro.encoder.feature_projection.projections.2.weight: found shape torch.Size([256, 512, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- depth_pro.encoder.feature_projection.projections.3.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.0.0.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.0.0.weight: found shape torch.Size([1024, 1024, 2, 2]) in the checkpoint and torch.Size([1024, 512, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.1.0.weight: found shape torch.Size([1024, 1024, 1, 1]) in the checkpoint and torch.Size([512, 1024, 1, 1]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.1.1.weight: found shape torch.Size([1024, 1024, 2, 2]) in the checkpoint and torch.Size([512, 512, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.2.0.weight: found shape torch.Size([1024, 1024, 1, 1]) in the checkpoint and torch.Size([256, 1024, 1, 1]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.2.1.weight: found shape torch.Size([1024, 1024, 2, 2]) in the checkpoint and torch.Size([256, 256, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.3.0.weight: found shape torch.Size([512, 1024, 1, 1]) in the checkpoint and torch.Size([128, 1024, 1, 1]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.3.1.weight: found shape torch.Size([512, 512, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.4.0.weight: found shape torch.Size([256, 1024, 1, 1]) in the checkpoint and torch.Size([128, 1024, 1, 1]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.4.1.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.4.2.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.5.0.weight: found shape torch.Size([256, 1024, 1, 1]) in the checkpoint and torch.Size([128, 1024, 1, 1]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.5.1.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.5.2.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.5.3.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.fuse_image_with_low_res.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- depth_pro.encoder.fuse_image_with_low_res.weight: found shape torch.Size([1024, 2048, 1, 1]) in the checkpoint and torch.Size([512, 1024, 1, 1]) in the model instantiated\n",
      "- depth_pro.encoder.image_encoder.embeddings.patch_embeddings.projection.weight: found shape torch.Size([1024, 3, 16, 16]) in the checkpoint and torch.Size([1024, 3, 4, 4]) in the model instantiated\n",
      "- depth_pro.encoder.image_encoder.embeddings.position_embeddings: found shape torch.Size([1, 577, 1024]) in the checkpoint and torch.Size([1, 65, 1024]) in the model instantiated\n",
      "- depth_pro.encoder.patch_encoder.embeddings.patch_embeddings.projection.weight: found shape torch.Size([1024, 3, 16, 16]) in the checkpoint and torch.Size([1024, 3, 4, 4]) in the model instantiated\n",
      "- depth_pro.encoder.patch_encoder.embeddings.position_embeddings: found shape torch.Size([1, 577, 1024]) in the checkpoint and torch.Size([1, 65, 1024]) in the model instantiated\n",
      "- fov_model.encoder.embeddings.patch_embeddings.projection.weight: found shape torch.Size([1024, 3, 16, 16]) in the checkpoint and torch.Size([1024, 3, 4, 4]) in the model instantiated\n",
      "- fov_model.encoder.embeddings.position_embeddings: found shape torch.Size([1, 577, 1024]) in the checkpoint and torch.Size([1, 65, 1024]) in the model instantiated\n",
      "- fov_model.encoder_neck.bias: found shape torch.Size([128]) in the checkpoint and torch.Size([64]) in the model instantiated\n",
      "- fov_model.encoder_neck.weight: found shape torch.Size([128, 1024]) in the checkpoint and torch.Size([64, 1024]) in the model instantiated\n",
      "- fov_model.global_neck.0.bias: found shape torch.Size([128]) in the checkpoint and torch.Size([64]) in the model instantiated\n",
      "- fov_model.global_neck.0.weight: found shape torch.Size([128, 256, 3, 3]) in the checkpoint and torch.Size([64, 128, 3, 3]) in the model instantiated\n",
      "- fov_model.head.0.bias: found shape torch.Size([64]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "- fov_model.head.0.weight: found shape torch.Size([64, 128, 3, 3]) in the checkpoint and torch.Size([32, 64, 3, 3]) in the model instantiated\n",
      "- fov_model.head.2.bias: found shape torch.Size([32]) in the checkpoint and torch.Size([16]) in the model instantiated\n",
      "- fov_model.head.2.weight: found shape torch.Size([32, 64, 3, 3]) in the checkpoint and torch.Size([16, 32, 3, 3]) in the model instantiated\n",
      "- fov_model.head.4.weight: found shape torch.Size([1, 32, 6, 6]) in the checkpoint and torch.Size([1, 16, 2, 2]) in the model instantiated\n",
      "- fusion_stage.layers.0.deconv.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- fusion_stage.layers.0.projection.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.0.projection.weight: found shape torch.Size([256, 256, 1, 1]) in the checkpoint and torch.Size([128, 128, 1, 1]) in the model instantiated\n",
      "- fusion_stage.layers.0.residual_layer1.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.0.residual_layer1.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.0.residual_layer1.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.0.residual_layer1.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.0.residual_layer2.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.0.residual_layer2.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.0.residual_layer2.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.0.residual_layer2.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.1.deconv.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- fusion_stage.layers.1.projection.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.1.projection.weight: found shape torch.Size([256, 256, 1, 1]) in the checkpoint and torch.Size([128, 128, 1, 1]) in the model instantiated\n",
      "- fusion_stage.layers.1.residual_layer1.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.1.residual_layer1.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.1.residual_layer1.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.1.residual_layer1.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.1.residual_layer2.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.1.residual_layer2.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.1.residual_layer2.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.1.residual_layer2.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.2.deconv.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- fusion_stage.layers.2.projection.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.2.projection.weight: found shape torch.Size([256, 256, 1, 1]) in the checkpoint and torch.Size([128, 128, 1, 1]) in the model instantiated\n",
      "- fusion_stage.layers.2.residual_layer1.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.2.residual_layer1.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.2.residual_layer1.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.2.residual_layer1.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.2.residual_layer2.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.2.residual_layer2.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.2.residual_layer2.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.2.residual_layer2.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.3.deconv.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- fusion_stage.layers.3.projection.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.3.projection.weight: found shape torch.Size([256, 256, 1, 1]) in the checkpoint and torch.Size([128, 128, 1, 1]) in the model instantiated\n",
      "- fusion_stage.layers.3.residual_layer1.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.3.residual_layer1.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.3.residual_layer1.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.3.residual_layer1.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.3.residual_layer2.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.3.residual_layer2.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.3.residual_layer2.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.3.residual_layer2.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.4.projection.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.4.projection.weight: found shape torch.Size([256, 256, 1, 1]) in the checkpoint and torch.Size([128, 128, 1, 1]) in the model instantiated\n",
      "- fusion_stage.layers.4.residual_layer1.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.4.residual_layer1.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.4.residual_layer1.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.4.residual_layer1.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.4.residual_layer2.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.4.residual_layer2.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.4.residual_layer2.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.4.residual_layer2.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- head.head.0.bias: found shape torch.Size([128]) in the checkpoint and torch.Size([64]) in the model instantiated\n",
      "- head.head.0.weight: found shape torch.Size([128, 256, 3, 3]) in the checkpoint and torch.Size([64, 128, 3, 3]) in the model instantiated\n",
      "- head.head.1.bias: found shape torch.Size([128]) in the checkpoint and torch.Size([64]) in the model instantiated\n",
      "- head.head.1.weight: found shape torch.Size([128, 128, 2, 2]) in the checkpoint and torch.Size([64, 64, 2, 2]) in the model instantiated\n",
      "- head.head.2.weight: found shape torch.Size([32, 128, 3, 3]) in the checkpoint and torch.Size([32, 64, 3, 3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/xiaoyanouyang/Library/Python/3.9/lib/python/site-packages/transformers/models/depth_pro/modeling_depth_pro.py:1018: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not (num_channels == self.config.num_channels):\n",
      "/Users/xiaoyanouyang/Library/Python/3.9/lib/python/site-packages/transformers/models/depth_pro/modeling_depth_pro.py:1023: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min(self.scaled_images_ratios) * min(height, width) < self.config.patch_size:\n",
      "/Users/xiaoyanouyang/Library/Python/3.9/lib/python/site-packages/transformers/models/depth_pro/modeling_depth_pro.py:879: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if height == width == patch_size:\n",
      "/Users/xiaoyanouyang/Library/Python/3.9/lib/python/site-packages/transformers/models/depth_pro/modeling_depth_pro.py:1048: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  n_patches_per_scaled_image = [len(i) for i in scaled_images]\n",
      "/Users/xiaoyanouyang/Library/Python/3.9/lib/python/site-packages/transformers/models/depth_pro/modeling_depth_pro.py:201: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_channels != self.num_channels:\n",
      "/Users/xiaoyanouyang/Library/Python/3.9/lib/python/site-packages/transformers/models/depth_pro/modeling_depth_pro.py:1092: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  exponent_value = int(math.log2(width / self.out_size))\n",
      "/Users/xiaoyanouyang/Library/Python/3.9/lib/python/site-packages/transformers/models/depth_pro/modeling_depth_pro.py:899: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  size = int(math.sqrt(seq_len))\n",
      "/Users/xiaoyanouyang/Library/Python/3.9/lib/python/site-packages/transformers/models/depth_pro/modeling_depth_pro.py:914: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  sqrt_n_patches_per_batch = int(math.sqrt(n_patches_per_batch))\n",
      "/Users/xiaoyanouyang/Library/Python/3.9/lib/python/site-packages/transformers/models/depth_pro/modeling_depth_pro.py:917: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if n_patches == batch_size:\n",
      "/Users/xiaoyanouyang/Library/Python/3.9/lib/python/site-packages/transformers/models/depth_pro/modeling_depth_pro.py:945: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if padding != 0:\n",
      "/Users/xiaoyanouyang/Library/Python/3.9/lib/python/site-packages/transformers/models/depth_pro/modeling_depth_pro.py:948: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  for index in starting_index:\n",
      "/Users/xiaoyanouyang/Library/Python/3.9/lib/python/site-packages/transformers/models/depth_pro/modeling_depth_pro.py:953: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  final_out_size = int(math.sqrt(merged.shape[-1]))\n"
     ]
    }
   ],
   "source": [
    "model = LightningModel(get_depthpro_model(32))\n",
    "state_dict = torch.load(\"../model_weights_10.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "onnx_model_path = \"./converted_onn_model_weights.onnx\"\n",
    "# dummy input - used to clarify the input shape\n",
    "dummy_input = torch.randn(1, 3, 224, 224)  \n",
    "torch.onnx.export(model, dummy_input, onnx_model_path,\n",
    "                  export_params=True, opset_version=20,\n",
    "                  do_constant_folding=True, input_names=['input'],\n",
    "                  output_names=['output'], dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}})\n",
    "\n",
    "print(f\"ONNX model saved to {onnx_model_path}\")\n",
    "\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d84e81-8b5e-4beb-b4ce-a7908258c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "ort_session.get_providers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec7d79-95ae-49e3-90cd-b8074c00f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = os.path.getsize(onnx_model_path) \n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddbc4cb-1b9f-4801-b844-179aff169e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef787559-bf0d-4671-8fe7-c0e94aa13084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
