{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f9bc70f-1a56-4139-9128-e4045c5ab3eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import numpy as np\n",
    "import lightning\n",
    "import pytorch_lightning \n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.quantization import quantize_dynamic\n",
    "import seaborn as sns\n",
    "import random\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"./mlflow-scripts\")\n",
    "from model import LightningModel, get_depthpro_model\n",
    "from dataloader import get_dataloaders, Urban100Dataset, collate_fn\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93efc775-0cc5-49ed-855d-5beec1881316",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model_weights_10.pth\"\n",
    "model = LightningModel(get_depthpro_model(32))\n",
    "checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "state_dict = checkpoint # [\"state_dict\"]  \n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()  \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d22981-dfe2-4087-abda-b7bbc7b52971",
   "metadata": {},
   "source": [
    "### model size on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84d84e81-8b5e-4beb-b4ce-a7908258c25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 654.93 MB\n"
     ]
    }
   ],
   "source": [
    "model_size = os.path.getsize(model_path) \n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b514c9-520b-4217-91eb-71f852d573cf",
   "metadata": {},
   "source": [
    "### Inference latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ddbc4cb-1b9f-4801-b844-179aff169e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_inference_latency(model):\n",
    "    test_dataset = Urban100Dataset()\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "    clear_output()\n",
    "    \n",
    "    def offline_eval(batch):\n",
    "        with torch.no_grad():\n",
    "            lr, hr = batch  # adjust depending on dataset output\n",
    "            lr = lr.to(\"cpu\")\n",
    "            hr = hr.to(\"cpu\")\n",
    "            sr = model(lr)\n",
    "            sr = F.interpolate(sr, size=hr.shape[2:])\n",
    "            return model.mse(sr, hr), model.psnr(sr, hr), model.ssim(sr, hr), model.snr(sr, hr)\n",
    "    \n",
    "    \n",
    "    latencies = []\n",
    "    count = 0\n",
    "    num_trials = 10\n",
    "    for batch in test_loader:\n",
    "        print(count)\n",
    "        if count > num_trials:\n",
    "            break\n",
    "        t1 = time.time()\n",
    "        _ = offline_eval(batch)\n",
    "        t2 = time.time()\n",
    "        latencies.append(t2-t1)\n",
    "        count += 1\n",
    "    clear_output() \n",
    "\n",
    "    print(f\"average inferenace latency is {np.mean(latencies)} seconds.\")\n",
    "    print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4b62637-1cb1-4a7b-9c6f-e8c47039147d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average inferenace latency is 8.018798221241344 seconds.\n",
      "Inference Latency (single sample, median): 4520.34 ms\n",
      "Inference Latency (single sample, 95th percentile): 21563.01 ms\n",
      "Inference Latency (single sample, 99th percentile): 25347.59 ms\n",
      "Inference Throughput (single sample): 0.11 FPS\n"
     ]
    }
   ],
   "source": [
    "compute_inference_latency(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0a738d-0a7e-4709-9161-bf4417262598",
   "metadata": {},
   "source": [
    "### Optimization with Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6900a209-5bcf-42ad-be1d-99c6a89245c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,  # the original model\n",
    "    {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
    "    dtype=torch.qint8  # the target dtype for quantized weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fcec53c-6de2-4659-be5c-5700c6af18ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average inferenace latency is 6.026599504730918 seconds.\n",
      "Inference Latency (single sample, median): 3908.45 ms\n",
      "Inference Latency (single sample, 95th percentile): 14256.32 ms\n",
      "Inference Latency (single sample, 99th percentile): 14458.82 ms\n",
      "Inference Throughput (single sample): 0.15 FPS\n"
     ]
    }
   ],
   "source": [
    "compute_inference_latency(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ecdf7f-e099-4edc-97fa-26ce2e9c6f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30dc9b6-80df-42ee-a3be-92e6b01c2dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
