{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370cd962-01a1-45ce-94e9-c3dfac1fa163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoyanouyang/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../mlflow-scripts\")\n",
    "from model import get_depthpro_model, LightningModel\n",
    "from dataloader import get_dataloaders, Urban100Dataset, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3d3f57-3709-423d-beae-6a99473ac50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DepthProForDepthEstimation were not initialized from the model checkpoint at geetu040/DepthPro and are newly initialized: ['depth_pro.encoder.feature_projection.projections.4.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.3.2.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.4.3.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.5.4.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.6.0.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.6.1.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.6.2.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.6.3.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.6.4.weight', 'depth_pro.encoder.feature_upsample.upsample_blocks.6.5.weight', 'fusion_stage.layers.4.deconv.weight', 'fusion_stage.layers.5.projection.bias', 'fusion_stage.layers.5.projection.weight', 'fusion_stage.layers.5.residual_layer1.convolution1.bias', 'fusion_stage.layers.5.residual_layer1.convolution1.weight', 'fusion_stage.layers.5.residual_layer1.convolution2.bias', 'fusion_stage.layers.5.residual_layer1.convolution2.weight', 'fusion_stage.layers.5.residual_layer2.convolution1.bias', 'fusion_stage.layers.5.residual_layer2.convolution1.weight', 'fusion_stage.layers.5.residual_layer2.convolution2.bias', 'fusion_stage.layers.5.residual_layer2.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DepthProForDepthEstimation were not initialized from the model checkpoint at geetu040/DepthPro and are newly initialized because the shapes did not match:\n",
      "- depth_pro.encoder.feature_projection.projections.0.weight: found shape torch.Size([256, 1024, 3, 3]) in the checkpoint and torch.Size([128, 512, 3, 3]) in the model instantiated\n",
      "- depth_pro.encoder.feature_projection.projections.1.weight: found shape torch.Size([256, 1024, 3, 3]) in the checkpoint and torch.Size([128, 256, 3, 3]) in the model instantiated\n",
      "- depth_pro.encoder.feature_projection.projections.2.weight: found shape torch.Size([256, 512, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- depth_pro.encoder.feature_projection.projections.3.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.0.0.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.0.0.weight: found shape torch.Size([1024, 1024, 2, 2]) in the checkpoint and torch.Size([1024, 512, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.1.0.weight: found shape torch.Size([1024, 1024, 1, 1]) in the checkpoint and torch.Size([512, 1024, 1, 1]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.1.1.weight: found shape torch.Size([1024, 1024, 2, 2]) in the checkpoint and torch.Size([512, 512, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.2.0.weight: found shape torch.Size([1024, 1024, 1, 1]) in the checkpoint and torch.Size([256, 1024, 1, 1]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.2.1.weight: found shape torch.Size([1024, 1024, 2, 2]) in the checkpoint and torch.Size([256, 256, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.3.0.weight: found shape torch.Size([512, 1024, 1, 1]) in the checkpoint and torch.Size([128, 1024, 1, 1]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.3.1.weight: found shape torch.Size([512, 512, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.4.0.weight: found shape torch.Size([256, 1024, 1, 1]) in the checkpoint and torch.Size([128, 1024, 1, 1]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.4.1.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.4.2.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.5.0.weight: found shape torch.Size([256, 1024, 1, 1]) in the checkpoint and torch.Size([128, 1024, 1, 1]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.5.1.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.5.2.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.feature_upsample.upsample_blocks.5.3.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- depth_pro.encoder.fuse_image_with_low_res.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([512]) in the model instantiated\n",
      "- depth_pro.encoder.fuse_image_with_low_res.weight: found shape torch.Size([1024, 2048, 1, 1]) in the checkpoint and torch.Size([512, 1024, 1, 1]) in the model instantiated\n",
      "- depth_pro.encoder.image_encoder.embeddings.patch_embeddings.projection.weight: found shape torch.Size([1024, 3, 16, 16]) in the checkpoint and torch.Size([1024, 3, 4, 4]) in the model instantiated\n",
      "- depth_pro.encoder.image_encoder.embeddings.position_embeddings: found shape torch.Size([1, 577, 1024]) in the checkpoint and torch.Size([1, 65, 1024]) in the model instantiated\n",
      "- depth_pro.encoder.patch_encoder.embeddings.patch_embeddings.projection.weight: found shape torch.Size([1024, 3, 16, 16]) in the checkpoint and torch.Size([1024, 3, 4, 4]) in the model instantiated\n",
      "- depth_pro.encoder.patch_encoder.embeddings.position_embeddings: found shape torch.Size([1, 577, 1024]) in the checkpoint and torch.Size([1, 65, 1024]) in the model instantiated\n",
      "- fov_model.encoder.embeddings.patch_embeddings.projection.weight: found shape torch.Size([1024, 3, 16, 16]) in the checkpoint and torch.Size([1024, 3, 4, 4]) in the model instantiated\n",
      "- fov_model.encoder.embeddings.position_embeddings: found shape torch.Size([1, 577, 1024]) in the checkpoint and torch.Size([1, 65, 1024]) in the model instantiated\n",
      "- fov_model.encoder_neck.bias: found shape torch.Size([128]) in the checkpoint and torch.Size([64]) in the model instantiated\n",
      "- fov_model.encoder_neck.weight: found shape torch.Size([128, 1024]) in the checkpoint and torch.Size([64, 1024]) in the model instantiated\n",
      "- fov_model.global_neck.0.bias: found shape torch.Size([128]) in the checkpoint and torch.Size([64]) in the model instantiated\n",
      "- fov_model.global_neck.0.weight: found shape torch.Size([128, 256, 3, 3]) in the checkpoint and torch.Size([64, 128, 3, 3]) in the model instantiated\n",
      "- fov_model.head.0.bias: found shape torch.Size([64]) in the checkpoint and torch.Size([32]) in the model instantiated\n",
      "- fov_model.head.0.weight: found shape torch.Size([64, 128, 3, 3]) in the checkpoint and torch.Size([32, 64, 3, 3]) in the model instantiated\n",
      "- fov_model.head.2.bias: found shape torch.Size([32]) in the checkpoint and torch.Size([16]) in the model instantiated\n",
      "- fov_model.head.2.weight: found shape torch.Size([32, 64, 3, 3]) in the checkpoint and torch.Size([16, 32, 3, 3]) in the model instantiated\n",
      "- fov_model.head.4.weight: found shape torch.Size([1, 32, 6, 6]) in the checkpoint and torch.Size([1, 16, 2, 2]) in the model instantiated\n",
      "- fusion_stage.layers.0.deconv.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- fusion_stage.layers.0.projection.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.0.projection.weight: found shape torch.Size([256, 256, 1, 1]) in the checkpoint and torch.Size([128, 128, 1, 1]) in the model instantiated\n",
      "- fusion_stage.layers.0.residual_layer1.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.0.residual_layer1.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.0.residual_layer1.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.0.residual_layer1.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.0.residual_layer2.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.0.residual_layer2.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.0.residual_layer2.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.0.residual_layer2.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.1.deconv.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- fusion_stage.layers.1.projection.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.1.projection.weight: found shape torch.Size([256, 256, 1, 1]) in the checkpoint and torch.Size([128, 128, 1, 1]) in the model instantiated\n",
      "- fusion_stage.layers.1.residual_layer1.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.1.residual_layer1.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.1.residual_layer1.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.1.residual_layer1.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.1.residual_layer2.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.1.residual_layer2.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.1.residual_layer2.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.1.residual_layer2.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.2.deconv.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- fusion_stage.layers.2.projection.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.2.projection.weight: found shape torch.Size([256, 256, 1, 1]) in the checkpoint and torch.Size([128, 128, 1, 1]) in the model instantiated\n",
      "- fusion_stage.layers.2.residual_layer1.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.2.residual_layer1.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.2.residual_layer1.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.2.residual_layer1.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.2.residual_layer2.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.2.residual_layer2.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.2.residual_layer2.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.2.residual_layer2.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.3.deconv.weight: found shape torch.Size([256, 256, 2, 2]) in the checkpoint and torch.Size([128, 128, 2, 2]) in the model instantiated\n",
      "- fusion_stage.layers.3.projection.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.3.projection.weight: found shape torch.Size([256, 256, 1, 1]) in the checkpoint and torch.Size([128, 128, 1, 1]) in the model instantiated\n",
      "- fusion_stage.layers.3.residual_layer1.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.3.residual_layer1.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.3.residual_layer1.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.3.residual_layer1.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.3.residual_layer2.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.3.residual_layer2.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.3.residual_layer2.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.3.residual_layer2.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.4.projection.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.4.projection.weight: found shape torch.Size([256, 256, 1, 1]) in the checkpoint and torch.Size([128, 128, 1, 1]) in the model instantiated\n",
      "- fusion_stage.layers.4.residual_layer1.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.4.residual_layer1.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.4.residual_layer1.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.4.residual_layer1.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.4.residual_layer2.convolution1.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.4.residual_layer2.convolution1.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- fusion_stage.layers.4.residual_layer2.convolution2.bias: found shape torch.Size([256]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- fusion_stage.layers.4.residual_layer2.convolution2.weight: found shape torch.Size([256, 256, 3, 3]) in the checkpoint and torch.Size([128, 128, 3, 3]) in the model instantiated\n",
      "- head.head.0.bias: found shape torch.Size([128]) in the checkpoint and torch.Size([64]) in the model instantiated\n",
      "- head.head.0.weight: found shape torch.Size([128, 256, 3, 3]) in the checkpoint and torch.Size([64, 128, 3, 3]) in the model instantiated\n",
      "- head.head.1.bias: found shape torch.Size([128]) in the checkpoint and torch.Size([64]) in the model instantiated\n",
      "- head.head.1.weight: found shape torch.Size([128, 128, 2, 2]) in the checkpoint and torch.Size([64, 64, 2, 2]) in the model instantiated\n",
      "- head.head.2.weight: found shape torch.Size([32, 128, 3, 3]) in the checkpoint and torch.Size([32, 64, 3, 3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LightningModel(\n",
       "  (model): DepthProForSuperResolution(\n",
       "    (depthpro_for_depth_estimation): DepthProForDepthEstimation(\n",
       "      (depth_pro): DepthProModel(\n",
       "        (encoder): DepthProEncoder(\n",
       "          (patch_encoder): DepthProViT(\n",
       "            (embeddings): DepthProViTEmbeddings(\n",
       "              (patch_embeddings): DepthProViTPatchEmbeddings(\n",
       "                (projection): Conv2d(3, 1024, kernel_size=(4, 4), stride=(4, 4))\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (encoder): DepthProViTEncoder(\n",
       "              (layer): ModuleList(\n",
       "                (0-3): 4 x DepthProViTLayer(\n",
       "                  (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                  (attention): DepthProViTSdpaAttention(\n",
       "                    (attention): DepthProViTSdpaSelfAttention(\n",
       "                      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): DepthProViTSelfOutput(\n",
       "                      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (layer_scale1): DepthProViTLayerScale()\n",
       "                  (drop_path): Identity()\n",
       "                  (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                  (mlp): DepthProViTMLP(\n",
       "                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                    (activation): GELUActivation()\n",
       "                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  )\n",
       "                  (layer_scale2): DepthProViTLayerScale()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (image_encoder): DepthProViT(\n",
       "            (embeddings): DepthProViTEmbeddings(\n",
       "              (patch_embeddings): DepthProViTPatchEmbeddings(\n",
       "                (projection): Conv2d(3, 1024, kernel_size=(4, 4), stride=(4, 4))\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (encoder): DepthProViTEncoder(\n",
       "              (layer): ModuleList(\n",
       "                (0-3): 4 x DepthProViTLayer(\n",
       "                  (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                  (attention): DepthProViTSdpaAttention(\n",
       "                    (attention): DepthProViTSdpaSelfAttention(\n",
       "                      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): DepthProViTSelfOutput(\n",
       "                      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (layer_scale1): DepthProViTLayerScale()\n",
       "                  (drop_path): Identity()\n",
       "                  (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                  (mlp): DepthProViTMLP(\n",
       "                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                    (activation): GELUActivation()\n",
       "                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                  )\n",
       "                  (layer_scale2): DepthProViTLayerScale()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (feature_upsample): DepthProFeatureUpsample(\n",
       "            (upsample_blocks): ModuleList(\n",
       "              (0): Sequential(\n",
       "                (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "              )\n",
       "              (2): Sequential(\n",
       "                (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "              )\n",
       "              (3): Sequential(\n",
       "                (0): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "                (2): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "              )\n",
       "              (4): Sequential(\n",
       "                (0): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "                (2): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "                (3): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "              )\n",
       "              (5): Sequential(\n",
       "                (0): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "                (2): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "                (3): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "                (4): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "              )\n",
       "              (6): Sequential(\n",
       "                (0): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "                (2): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "                (3): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "                (4): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "                (5): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (fuse_image_with_low_res): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (feature_projection): DepthProFeatureProjection(\n",
       "            (projections): ModuleList(\n",
       "              (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (2-4): 3 x Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (5): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (fusion_stage): DepthProFeatureFusionStage(\n",
       "        (layers): ModuleList(\n",
       "          (0-4): 5 x DepthProFeatureFusionLayer(\n",
       "            (residual_layer1): DepthProPreActResidualLayer(\n",
       "              (activation1): ReLU()\n",
       "              (convolution1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation2): ReLU()\n",
       "              (convolution2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (residual_layer2): DepthProPreActResidualLayer(\n",
       "              (activation1): ReLU()\n",
       "              (convolution1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation2): ReLU()\n",
       "              (convolution2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (deconv): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "            (projection): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (5): DepthProFeatureFusionLayer(\n",
       "            (residual_layer1): DepthProPreActResidualLayer(\n",
       "              (activation1): ReLU()\n",
       "              (convolution1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation2): ReLU()\n",
       "              (convolution2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (residual_layer2): DepthProPreActResidualLayer(\n",
       "              (activation1): ReLU()\n",
       "              (convolution1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (activation2): ReLU()\n",
       "              (convolution2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "            (projection): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (skip_add): FloatFunctional(\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (head): DepthProDepthEstimationHead(\n",
       "        (head): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): ReLU(inplace=True)\n",
       "          (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (fov_model): DepthProFOVModel(\n",
       "        (encoder): DepthProViT(\n",
       "          (embeddings): DepthProViTEmbeddings(\n",
       "            (patch_embeddings): DepthProViTPatchEmbeddings(\n",
       "              (projection): Conv2d(3, 1024, kernel_size=(4, 4), stride=(4, 4))\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (encoder): DepthProViTEncoder(\n",
       "            (layer): ModuleList(\n",
       "              (0-3): 4 x DepthProViTLayer(\n",
       "                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (attention): DepthProViTSdpaAttention(\n",
       "                  (attention): DepthProViTSdpaSelfAttention(\n",
       "                    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): DepthProViTSelfOutput(\n",
       "                    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (layer_scale1): DepthProViTLayerScale()\n",
       "                (drop_path): Identity()\n",
       "                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): DepthProViTMLP(\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (activation): GELUActivation()\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "                (layer_scale2): DepthProViTLayerScale()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (encoder_neck): Linear(in_features=1024, out_features=64, bias=True)\n",
       "        (global_neck): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (head): Sequential(\n",
       "          (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (3): ReLU(inplace=True)\n",
       "          (4): Conv2d(16, 1, kernel_size=(2, 2), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (image_head): Sequential(\n",
       "      (0): ConvTranspose2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (snr): SignalNoiseRatio()\n",
       "  (psnr): PeakSignalNoiseRatio()\n",
       "  (ssim): StructuralSimilarityIndexMeasure()\n",
       "  (mse): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_path = \"best_model.pth\"  \n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#model = torch.load(model_path, map_location=device, weights_only=False)\n",
    "# _ = model.eval()  \n",
    "\n",
    "\n",
    "model = LightningModel(get_depthpro_model(32))\n",
    "checkpoint = torch.load(\"../model_weights_10.pth\", map_location=\"cpu\")\n",
    "state_dict = checkpoint # [\"state_dict\"]  # Lightning 会把实际模型参数放在这个 key 下\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce04e0ce-e27f-4135-a3e9-0816122730cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_images_path: ['../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_009_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_049_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_078_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_087_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_080_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_095_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_038_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_092_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_048_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_008_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_093_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_039_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_094_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_081_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_086_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_079_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_026_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_073_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_021_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_074_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_034_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_061_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_033_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_066_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_099_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_005_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_050_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_002_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_057_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_017_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_042_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_010_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_045_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_098_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_067_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_032_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_060_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_035_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_075_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_020_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_072_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_027_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_044_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_011_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_043_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_016_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_056_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_003_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_051_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_004_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_037_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_062_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_030_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_065_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_025_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_070_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_100_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_088_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_022_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_077_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_014_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_041_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_013_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_046_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_006_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_053_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_001_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_054_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_076_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_023_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_089_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_071_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_024_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_064_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_031_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_063_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_036_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_055_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_052_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_007_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_047_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_012_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_040_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_015_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_018_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_058_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_096_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_069_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_091_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_084_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_083_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_029_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_059_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_019_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_028_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_082_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_085_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_090_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_068_SRF_4_LR.png', '../dataset/Urban 100/X4 Urban100/X4/LOW x4 URban100/img_097_SRF_4_LR.png']\n",
      "hr_images_path: ['../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_054_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_001_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_053_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_006_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_046_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_013_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_041_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_014_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_088_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_077_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_022_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_100_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_070_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_025_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_065_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_030_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_062_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_037_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_015_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_040_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_012_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_047_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_007_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_052_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_055_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_036_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_063_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_031_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_064_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_024_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_071_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_023_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_076_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_089_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_083_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_029_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_084_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_091_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_096_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_069_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_058_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_018_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_068_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_097_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_090_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_085_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_028_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_082_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_019_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_059_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_038_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_092_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_095_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_080_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_078_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_087_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_049_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_009_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_086_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_079_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_081_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_094_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_093_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_039_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_008_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_048_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_045_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_010_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_042_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_017_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_057_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_002_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_050_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_005_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_066_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_033_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_099_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_061_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_034_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_074_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_021_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_073_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_026_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_004_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_051_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_003_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_056_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_016_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_043_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_011_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_044_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_027_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_072_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_020_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_075_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_035_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_060_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_098_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_032_SRF_4_HR.png', '../dataset/Urban 100/X4 Urban100/X4/HIGH x4 URban100/img_067_SRF_4_HR.png']\n"
     ]
    }
   ],
   "source": [
    "test_dataset = Urban100Dataset()\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f7e86b-3eae-4903-96aa-99137c032151",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing img 0 out of 100\n",
      "processing img 1 out of 100\n",
      "processing img 2 out of 100\n",
      "processing img 3 out of 100\n",
      "processing img 4 out of 100\n",
      "processing img 5 out of 100\n",
      "processing img 6 out of 100\n",
      "processing img 7 out of 100\n",
      "processing img 8 out of 100\n",
      "processing img 9 out of 100\n",
      "processing img 10 out of 100\n",
      "processing img 11 out of 100\n",
      "processing img 12 out of 100\n",
      "processing img 13 out of 100\n",
      "processing img 14 out of 100\n",
      "processing img 15 out of 100\n",
      "processing img 16 out of 100\n",
      "processing img 17 out of 100\n",
      "processing img 18 out of 100\n",
      "processing img 19 out of 100\n",
      "processing img 20 out of 100\n",
      "processing img 21 out of 100\n",
      "processing img 22 out of 100\n",
      "processing img 23 out of 100\n",
      "processing img 24 out of 100\n",
      "processing img 25 out of 100\n",
      "processing img 26 out of 100\n",
      "processing img 27 out of 100\n",
      "processing img 28 out of 100\n",
      "processing img 29 out of 100\n",
      "processing img 30 out of 100\n",
      "processing img 31 out of 100\n",
      "processing img 32 out of 100\n",
      "processing img 33 out of 100\n",
      "processing img 34 out of 100\n",
      "processing img 35 out of 100\n",
      "processing img 36 out of 100\n",
      "processing img 37 out of 100\n",
      "processing img 38 out of 100\n",
      "processing img 39 out of 100\n",
      "processing img 40 out of 100\n",
      "processing img 41 out of 100\n",
      "processing img 42 out of 100\n",
      "processing img 43 out of 100\n",
      "processing img 44 out of 100\n",
      "processing img 45 out of 100\n",
      "processing img 46 out of 100\n",
      "processing img 47 out of 100\n",
      "processing img 48 out of 100\n",
      "processing img 49 out of 100\n",
      "processing img 50 out of 100\n",
      "processing img 51 out of 100\n",
      "processing img 52 out of 100\n",
      "processing img 53 out of 100\n",
      "processing img 54 out of 100\n",
      "processing img 55 out of 100\n",
      "processing img 56 out of 100\n",
      "processing img 57 out of 100\n",
      "processing img 58 out of 100\n",
      "processing img 59 out of 100\n",
      "processing img 60 out of 100\n",
      "processing img 61 out of 100\n",
      "processing img 62 out of 100\n",
      "processing img 63 out of 100\n",
      "processing img 64 out of 100\n",
      "processing img 65 out of 100\n",
      "processing img 66 out of 100\n",
      "processing img 67 out of 100\n",
      "processing img 68 out of 100\n",
      "processing img 69 out of 100\n",
      "processing img 70 out of 100\n",
      "processing img 71 out of 100\n",
      "processing img 72 out of 100\n",
      "processing img 73 out of 100\n",
      "processing img 74 out of 100\n",
      "processing img 75 out of 100\n",
      "processing img 76 out of 100\n",
      "processing img 77 out of 100\n",
      "processing img 78 out of 100\n",
      "processing img 79 out of 100\n",
      "processing img 80 out of 100\n",
      "processing img 81 out of 100\n",
      "processing img 82 out of 100\n",
      "processing img 83 out of 100\n",
      "processing img 84 out of 100\n",
      "processing img 85 out of 100\n",
      "processing img 86 out of 100\n",
      "processing img 87 out of 100\n",
      "processing img 88 out of 100\n",
      "processing img 89 out of 100\n",
      "processing img 90 out of 100\n",
      "processing img 91 out of 100\n",
      "processing img 92 out of 100\n",
      "processing img 93 out of 100\n",
      "processing img 94 out of 100\n",
      "processing img 95 out of 100\n",
      "processing img 96 out of 100\n",
      "processing img 97 out of 100\n",
      "processing img 98 out of 100\n",
      "processing img 99 out of 100\n",
      "mse: 0.609491\n",
      "psnr: 8.336657\n",
      "ssim: 0.017536396\n",
      "snr: -2.5604413\n"
     ]
    }
   ],
   "source": [
    "def offline_eval(batch):\n",
    "    with torch.no_grad():\n",
    "        lr, hr = batch  # adjust depending on dataset output\n",
    "        lr = lr.to(\"cpu\")\n",
    "        hr = hr.to(\"cpu\")\n",
    "        sr = model(lr)\n",
    "        sr = F.interpolate(sr, size=hr.shape[2:])\n",
    "        return model.mse(sr, hr), model.psnr(sr, hr), model.ssim(sr, hr), model.snr(sr, hr)\n",
    "\n",
    "\n",
    "mse_list, psnr_list, ssim_list, snr_list = [], [], [], []\n",
    "count = 0\n",
    "for batch in test_loader:\n",
    "    print(f\"processing img {count} out of {test_dataset.__len__()}\")\n",
    "    mse, psnr, ssim, snr = offline_eval(batch)\n",
    "    count += 1\n",
    "    mse_list.append(mse)\n",
    "    psnr_list.append(psnr)\n",
    "    ssim_list.append(ssim)\n",
    "    snr_list.append(snr)\n",
    "\n",
    "print(\"mse:\", np.mean(mse_list))\n",
    "print(\"psnr:\", np.mean(psnr_list))\n",
    "print(\"ssim:\", np.mean(ssim_list))\n",
    "print(\"snr:\", np.mean(snr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f42c5d-8663-42f7-a774-1d9757a0c66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a24e03-0fff-466f-9d72-9429ceeb6188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e3653-4b49-44f4-a3d3-97650d57f6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b750b4-1e08-47e5-9b83-56610698b4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d90b2b-82a6-4014-a83a-9f80c9d60baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0891567-db38-4539-bce1-eda2aa8921c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b36ef5-84e0-4027-bbb1-4aefe2c5c9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
