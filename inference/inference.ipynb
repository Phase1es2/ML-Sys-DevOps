{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1db6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install -q numpy pillow torch torchvision\n",
    "pip install -q git+https://github.com/geetu040/transformers.git@depth-pro-projects#egg=transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793c66b1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# custom installation from this PR: https://github.com/huggingface/transformers/pull/34583\n",
    "# !pip install git+https://github.com/geetu040/transformers.git@depth-pro-projects#egg=transformers\n",
    "from transformers import DepthProConfig, DepthProImageProcessorFast, DepthProForDepthEstimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d43e99",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# load DepthPro model, used as backbone\n",
    "config = DepthProConfig(\n",
    "    patch_size=32,\n",
    "    patch_embeddings_size=4,\n",
    "    num_hidden_layers=12,\n",
    "    intermediate_hook_ids=[11, 8, 7, 5],\n",
    "    intermediate_feature_dims=[256, 256, 256, 256],\n",
    "    scaled_images_ratios=[0.5, 1.0],\n",
    "    scaled_images_overlap_ratios=[0.5, 0.25],\n",
    "    scaled_images_feature_dims=[1024, 512],\n",
    "    use_fov_model=False,\n",
    ")\n",
    "depthpro_for_depth_estimation = DepthProForDepthEstimation(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1153bca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# create DepthPro for super resolution\n",
    "class DepthProForSuperResolution(torch.nn.Module):\n",
    "    def __init__(self, depthpro_for_depth_estimation):\n",
    "        super().__init__()\n",
    "\n",
    "        self.depthpro_for_depth_estimation = depthpro_for_depth_estimation\n",
    "        hidden_size = self.depthpro_for_depth_estimation.config.fusion_hidden_size\n",
    "\n",
    "        self.image_head = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(\n",
    "                in_channels=config.num_channels,\n",
    "                out_channels=hidden_size,\n",
    "                kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.head = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=hidden_size,\n",
    "                out_channels=hidden_size,\n",
    "                kernel_size=3, stride=1, padding=1\n",
    "            ),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(\n",
    "                in_channels=hidden_size,\n",
    "                out_channels=hidden_size,\n",
    "                kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=hidden_size,\n",
    "                out_channels=self.depthpro_for_depth_estimation.config.num_channels,\n",
    "                kernel_size=3, stride=1, padding=1\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # x is the low resolution image\n",
    "        x = pixel_values\n",
    "        encoder_features = self.depthpro_for_depth_estimation.depth_pro(x).features\n",
    "        fused_hidden_state = self.depthpro_for_depth_estimation.fusion_stage(encoder_features)[-1]\n",
    "        x = self.image_head(x)\n",
    "        x = torch.nn.functional.interpolate(x, size=fused_hidden_state.shape[2:])\n",
    "        x = x + fused_hidden_state\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d5bf7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = DepthProForSuperResolution(depthpro_for_depth_estimation)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# load weights\n",
    "weights_path = hf_hub_download(repo_id=\"geetu040/DepthPro_SR_4x_256p\", filename=\"model_weights.pth\")\n",
    "model.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# load image processor\n",
    "image_processor = DepthProImageProcessorFast(\n",
    "    do_resize=False,\n",
    "    do_rescale=True,\n",
    "    do_normalize=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d4d9bf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# inference\n",
    "\n",
    "url = \"enter an images url\"\n",
    "# path = \"enter an image path\"\n",
    "\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "# image = Image.open(requests.get(path, stream=True).raw)\n",
    "image.thumbnail((256, 256)) # resizes the image object to fit within a 256x256 pixel box\n",
    "\n",
    "# prepare image for the model\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# convert tensors to PIL.Image\n",
    "output = outputs[0]                        # extract the first and only batch\n",
    "output = output.cpu()                      # unload from cuda if used\n",
    "output = torch.permute(output, (1, 2, 0))  # (C, H, W) -> (H, W, C)\n",
    "output = output * 0.5 + 0.5                # undo normalization\n",
    "output = output * 255.                     # undo scaling\n",
    "output = output.clip(0, 255.)              # fix out of range\n",
    "output = output.numpy()                    # convert to numpy\n",
    "output = output.astype('uint8')            # convert to PIL.Image compatible format\n",
    "output = Image.fromarray(output)           # create PIL.Image object\n",
    "\n",
    "# visualize the prediction\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 20))\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title(f'Low-Resolution (LR) {image.size}')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(output)\n",
    "axes[1].set_title(f'Super-Resolution (SR) {output.size}')\n",
    "axes[1].axis('off')\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
